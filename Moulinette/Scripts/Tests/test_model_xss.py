###########################################################################@
# This script is for testing the model_xss.h5 file generated by generate_model_xss.py
###########################################################################@
import pandas as pd
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

###########################################################################@
# Parameters 
###########################################################################@

#Paths
token_path = './IA/Tokens/xss.tokens'
model_path = './IA/Models/model_xss.h5'

#Model parameters
paranoia = 0.5
vocab_size = 8000
max_length = 300
embedding_dim = 16

###########################################################################@
# Testing the model
###########################################################################@

#1.Load the tokenizer
with open(token_path, 'rb') as file:
    tokenizer = pickle.load(file)
#2.Load the model
model = load_model(model_path)

#3. Predict function
def verify_query(query):
    query_sequence = tokenizer.texts_to_sequences([query])
    padded_sequence = pad_sequences(query_sequence, maxlen=max_length, padding='post', truncating='post')
    prediction = model.predict(padded_sequence)
    result = prediction[0][0]
    if result > paranoia:
        return str(result) + " Verdict: MALICIOUS", "MALICIOUS"
    else:
        return str(result) + " Verdict: SAFE", "SAFE"

#4. List of SAFE queries to test
print("\n###############################################@")
print(" Testing the model with SAFE queries")
print("###############################################@")
queries_known_as_safe = [
    "?search=hello",
    "?page=contact",
    "?user_id=1234",
    "?product_name=phone",
    "?action=update_profile&name=JohnDoe",
    "?category=books&sort=asc",
    "?article_id=56&author=Jane",
    "?cart=add&item=book",
    "?page=login",
    "?search_term=computer&filter=price_asc",
    "?order_id=56789",
    "?page=home&theme=dark",
    "?user_profile=view&user=alice",
    "?date=2024-10-15",
    "?city=Paris&temperature=23",
    "?session_id=abc123&timeout=30",
    "?lang=en&currency=USD",
    "?page=about&team_member=jack",
    "?product_id=9999&action=add_to_cart",
    "?video=12345&quality=1080p",
    "?report_id=2024&export=pdf"
]


safe_correct = 0
for query_text in queries_known_as_safe:
    result, verdict = verify_query(query_text)
    print(f"This query is considered as : {result}")
    if verdict == "SAFE":
        safe_correct += 1

#5. List of MALICIOUS queries to test
print("\n###############################################@")
print(" Testing the model with MALICIOUS queries")
print("###############################################@")
queries_known_as_malicious = [
    "?search=<script></script>",
    "?user=<img src=x onerror=alert(1)>",
    "?comment=<svg/onload=alert('XSS')>",
    "?name=<body onload=alert('XSS')>",
    "?message=<iframe src=javascript:alert(1)>",
    "?input=<input onfocus=alert(1)>",
    "?query=<a href=\"javascript:alert(1)\">Click here</a>",
    "?feedback=<embed src=\"javascript:alert(1)\">",
    "?redirect_url=javascript:alert('XSS')",
    "?form=<div style=\"background:url(javascript:alert(1))\">",
    "?search=<iframe src=javascript:alert('XSS')>",
    "?page=<img src=invalid onerror=alert(1)>",
    "?message=<object type=\"text/html\" data=\"javascript:alert('XSS')\">",
    "?file=<a href=\"javascript:alert('XSS')\">File</a>",
    "?user_input=<button onclick=alert(1)>Click me</button>",
    "?popup=<embed src=javascript:alert('XSS')>",
    "?input_value=<div style=background:url('javascript:alert(1)')>",
    "?tag=<iframe onload=alert('XSS')>",
    "?header=<meta http-equiv=\"refresh\" content=\"0; url=javascript:alert(1)\">",
    "?onclick=<a href=javascript:alert(1)>Click here</a>"
]

malicious_correct = 0
for query_text in queries_known_as_malicious:
    result, verdict = verify_query(query_text)
    print(f"This query is considered as : {result}")
    if verdict == "MALICIOUS":
        malicious_correct += 1

#6. Calculate and print statistics
total_safe = len(queries_known_as_safe)
total_malicious = len(queries_known_as_malicious)

safe_accuracy = (safe_correct / total_safe) * 100
malicious_accuracy = (malicious_correct / total_malicious) * 100

print("\n###############################################@")
print(" Statistics")
print("###############################################@")
print(f"Safe queries success rate: {safe_accuracy:.2f}%")
print(f"Malicious queries success rate: {malicious_accuracy:.2f}%")
print("")